文件读写
    f_in = ''
    with open(f_in, mode='r', encoding='utf-8') as rf:
        for l in rf.readlines():

-
        with open(f2, mode='w', encoding='utf-8') as o1:
            for item in new_ls:
                o1.write(item + '\n')



 2. 加载和保存一个模型


     def save_to_file(self, filename,obj):
        with open(filename, 'wb') as f:
            pickle.dump(obj, f)

        if filename is not None:
            with open(filename, 'rb') as f:
                obj = pickle.load(f)
        return obj


  3. 排序
      for index in range(len(result)):
        tmp = result[index]
        tmp = sorted(tmp, key=bkh.get_total)
        result[index] = tmp


            tp = ct.sort_dict(d_dict, True)
            for t in tp:
                out.write("%s\t%s\n" % (t[0], t[1]))



    # def __init__(self):
    #     # jieba.set_dictionary('../data/jieba_dict/dict.txt.big')
    #     # self.stopwordset = set()
    #     # with open('../data/jieba_dict/stopwords.txt', 'r', encoding='utf-8') as sw:
    #     #     for line in sw:
    #     #         self.stopwordset.add(line.strip('\n'))
    #     print(1)

    #
    # 输入文本，输出分词后的文本
    # type = rdf | questions
    # def convert_text_to_seg(self, file_in, file_out, type="rdf"):
    #     with open(file_out, 'w', encoding='utf-8') as f_out:
    #         with open(file_in, 'r', encoding='utf-8') as f_in:
    #             for line in f_in:
    #                 if type == "rdf":
    #                     # 增加操作
    #                     print(124444)
    #                 if type == "questions":
    #                     line = line.split('\t')[0]
    #                 line = line.strip('\n')
    #                 words = jieba.cut(line, cut_all=False)
    #
    #                 words_out = []
    #                 for word in words:
    #                     if word not in self.stopwordset:
    #                         words_out.append(word)
    #                 f_out.write(' '.join(words_out) + '\n')
    #     print(321321)