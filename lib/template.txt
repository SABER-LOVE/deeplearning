文件读写
    f_in = ''
    _index = -1
    with open(f_in, mode='r', encoding='utf-8') as rf:
        for l in rf:
            _index += 1
            if _index % 10000 == 0:
                ct.print(_index/10000)

-
        with open(f2, mode='w', encoding='utf-8') as o1:
            for item in new_ls:
                o1.write(item + '\n')


f1s = ct.file_read_all_lines_strip('../data/nlpcc2016/3-questions/q.rdf.m_s.filter.txt')

        # 问题0 答案1 实体s-2 关系p-3 属性值o-4    匹配到的实体s-5

                vs = self.kbqa.get(f3s_e, "")
                if vs == '':
                    ct.just_log('../data/nlpcc2016/demo1/extract_kb.log.txt', f3s_e)
                    print(f3s_e)
                    continue
                for po in vs:
                    msg = "%s\t%s\t%s" % (f3s_e, po[0], po[1])
                    o1.write(msg + '\n')


r3 = (r_train | r_test) - r_train

for k,v in d1.items():
    print(k)
    print(v)


 2. 加载和保存一个模型


     def save_to_file(self, filename,obj):
        with open(filename, 'wb') as f:
            pickle.dump(obj, f)

        if filename is not None:
            with open(filename, 'rb') as f:
                obj = pickle.load(f)
        return obj


  3. 排序
      for index in range(len(result)):
        tmp = result[index]
        tmp = sorted(tmp, key=bkh.get_total)
        result[index] = tmp


            tp = ct.sort_dict(d_dict, True)
            for t in tp:
                out.write("%s\t%s\n" % (t[0], t[1]))

            tp = ct.sort_dict(extract_dict, True)
            f5s = []
            for t in tp:
                f5s.append("%s\t%s" % (t[0], t[1]))
            ct.file_wirte_list('',f5s)


            for k in d1.keys():
                 msg = "%s\t%s\t%s" % (last_s, k, '\t'.join(d1[k]))
                  f4s.append(msg)
            ct.file_wirte_list('',f5s)

    # def __init__(self):
    #     # jieba.set_dictionary('../data/jieba_dict/dict.txt.big')
    #     # self.stopwordset = set()
    #     # with open('../data/jieba_dict/stopwords.txt', 'r', encoding='utf-8') as sw:
    #     #     for line in sw:
    #     #         self.stopwordset.add(line.strip('\n'))
    #     print(1)

    #
    # 输入文本，输出分词后的文本
    # type = rdf | questions
    # def convert_text_to_seg(self, file_in, file_out, type="rdf"):
    #     with open(file_out, 'w', encoding='utf-8') as f_out:
    #         with open(file_in, 'r', encoding='utf-8') as f_in:
    #             for line in f_in:
    #                 if type == "rdf":
    #                     # 增加操作
    #                     print(124444)
    #                 if type == "questions":
    #                     line = line.split('\t')[0]
    #                 line = line.strip('\n')
    #                 words = jieba.cut(line, cut_all=False)
    #
    #                 words_out = []
    #                 for word in words:
    #                     if word not in self.stopwordset:
    #                         words_out.append(word)
    #                 f_out.write(' '.join(words_out) + '\n')
    #     print(321321)



    tmp = sorted(tmp, key=bkh.get_total)